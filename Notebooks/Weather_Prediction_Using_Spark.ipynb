{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baeddbd-2cac-4227-a420-fb5c34189ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3.6\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfb2c9-e1a7-4273-95eb-244b513d54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"WeatherPredictionPune\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906486f7-8daf-40d2-8369-ad8f456c1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Spark DataFrame\n",
    "df_spark = spark.read.csv(\"file:///home/talentum/Pune.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3763f-f571-48fa-a9fb-7af2d08ff356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "# Plotting\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig, axs = plt.subplots(2, 3, figsize=(10, 8))\n",
    "\n",
    "sns.histplot(data=df, x=\"maxtempC\", kde=True, ax=axs[0, 0], color='red')\n",
    "sns.histplot(data=df, x=\"mintempC\", kde=True, ax=axs[0, 1], color='green')\n",
    "sns.histplot(data=df, x=\"pressure\", kde=True, ax=axs[0, 2], color='blue')\n",
    "sns.histplot(data=df, x=\"humidity\", kde=True, ax=axs[1, 0], color='orange')\n",
    "sns.histplot(data=df, x=\"HeatIndexC\", kde=True, ax=axs[1, 1], color='black')\n",
    "sns.histplot(data=df, x=\"uvIndex\", kde=True, ax=axs[1, 2], color='yellow')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('histogram_distribution.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2076c64-3e21-4a55-8cd1-f7f6e07edd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows\n",
    "num_rows = df_spark.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(df_spark.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e40e61-748d-41ce-a69d-1aa52b3bf01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96bd45-eb66-429d-a995-9c2746c1f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Print the schema\n",
    "\n",
    "print(\"Schema:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Print column names\n",
    "print(\"\\nColumns:\")\n",
    "print(df_spark.columns)\n",
    "\n",
    "# Count number of rows\n",
    "num_rows = df_spark.count()\n",
    "print(\"\\nNumber of rows:\", num_rows)\n",
    "\n",
    "# Show basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "df_spark.describe().show()\n",
    "\n",
    "# Count nulls in each column\n",
    "print(\"\\nNull counts:\")\n",
    "null_counts = df_spark.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_spark.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a3e8f-2cbf-458c-8e69-2c1efb307cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd75c4-8f6d-4233-a5bb-5dd533e62529",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['date_time', 'moonrise', 'moonset', 'sunrise', 'sunset', 'FeelsLikeC']\n",
    "df_spark = df_spark.drop(*columns_to_drop)\n",
    "\n",
    "# Show the updated DataFrame schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830d0d1-185b-4802-bc3a-cc24f768d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a list of column names with corresponding null counts\n",
    "null_counts = df_spark.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_spark.columns])\n",
    "\n",
    "# Show the null counts\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552e791-6469-4613-a061-f044722c3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, col, sum\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_columns = [c for c, dtype in df_spark.dtypes if dtype in ('int', 'double')]\n",
    "\n",
    "# Calculate mean for each numerical column\n",
    "mean_values = df_spark.select([mean(col(c)).alias(c) for c in numerical_columns]).first().asDict()\n",
    "\n",
    "# Fill missing values with the calculated means\n",
    "filled_df = df_spark.fillna(mean_values)\n",
    "\n",
    "# Count nulls in each column after filling\n",
    "null_counts = filled_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in filled_df.columns])\n",
    "\n",
    "# Show the null counts to verify\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0ba8e-1795-4a0b-bf72-e05c31e9019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'tempC' column to create X\n",
    "X = filled_df.drop('tempC')\n",
    "\n",
    "# Select the 'tempC' column to create Y\n",
    "Y = filled_df.select('tempC')\n",
    "\n",
    "# Show X and Y to verify\n",
    "X.show()\n",
    "Y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f7e3c-8e0c-4ce2-86ee-7706a9c75c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "\n",
    "train_df, test_df = filled_df.randomSplit([0.8, 0.2], seed=69)\n",
    "\n",
    "# HDFS namenode address and port\n",
    "namenode = \"hdfs://localhost\"\n",
    "port = \"9000\"\n",
    "\n",
    "# Full HDFS path\n",
    "hdfs_path = f\"{namenode}:{port}/user/talentum/\"\n",
    "\n",
    "# Save the training and testing sets to HDFS\n",
    "train_df.write.mode('overwrite').parquet(f'{hdfs_path}/train_df')\n",
    "test_df.write.mode('overwrite').parquet(f'{hdfs_path}/test_df')\n",
    "\n",
    "# Separate features and target in training and testing sets\n",
    "X_train = train_df.drop('tempC')\n",
    "Y_train = train_df.select('tempC')\n",
    "X_test = test_df.drop('tempC')\n",
    "Y_test = test_df.select('tempC')\n",
    "\n",
    "# Save the training and testing sets to HDFS\n",
    "X_train.write.mode('overwrite').parquet(f'{hdfs_path}/X_train')\n",
    "Y_train.write.mode('overwrite').parquet(f'{hdfs_path}/Y_train')\n",
    "X_test.write.mode('overwrite').parquet(f'{hdfs_path}/X_test')\n",
    "Y_test.write.mode('overwrite').parquet(f'{hdfs_path}/Y_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57563e-b9d6-4b5b-94fa-3389641257ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS namenode address and port\n",
    "namenode = \"hdfs://localhost\"\n",
    "port = \"9000\"\n",
    "\n",
    "# Full HDFS path\n",
    "hdfs_path = f\"{namenode}:{port}/user/talentum/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8f476-38af-4ff7-a1b9-23958f6da0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the training and testing sets from HDFS\n",
    "train_df = spark.read.parquet(f'{hdfs_path}/train_df')\n",
    "test_df = spark.read.parquet(f'{hdfs_path}/test_df')\n",
    "\n",
    "# Prepare the feature assembler\n",
    "assembler = VectorAssembler(inputCols=[col for col in train_df.columns if col != 'tempC'], outputCol=\"features\")\n",
    "\n",
    "# Transform training and test data\n",
    "train_data = assembler.transform(train_df).select(\"features\", col(\"tempC\").alias(\"label\"))\n",
    "test_data = assembler.transform(test_df).select(\"features\", col(\"tempC\").alias(\"label\"))\n",
    "\n",
    "# Initialize the Linear Regression model with maxIter set to 50\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# Set up the parameter grid for tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 0.5])         # Regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])   # ElasticNet mixing parameter\n",
    "             .build())\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Set up cross-validator with parallelism\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,      # 3-fold cross-validation\n",
    "                          parallelism=4)   # Use 4 parallel tasks\n",
    "\n",
    "# Fit the cross-validator model\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Select the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_predictions = best_model.transform(test_data)\n",
    "rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Best Model's RMSE on test data: {rmse}\")\n",
    "\n",
    "# Define the HDFS model path\n",
    "\n",
    "model_path = f\"{namenode}:{port}/user/talentum/lrg_best_model\"\n",
    "\n",
    "# Save the best model to HDFS with overwrite option\n",
    "try:\n",
    "    best_model.write().overwrite().save(model_path)\n",
    "    print(f\"Best model saved to: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8a777-1054-4be5-b593-4c2af0325ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Initialize the RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"  # Mean Absolute Error\n",
    ")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e68a5b-b869-4dbc-89f2-6cc49aadb429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Initialize the RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mse\"  # Mean Squared Error\n",
    ")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1bb11e-176d-4b45-91d7-7530673241f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Initialize the RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"  # R-squared score\n",
    ")\n",
    "\n",
    "# Calculate R-squared (R²) score\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R-squared (R²) score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcdb1a-ade0-410a-a5ee-4b7e32d5a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Extract predictions as a DataFrame\n",
    "predicted_values_df = predictions.select(\"prediction\")\n",
    "\n",
    "# Collect predictions into a list\n",
    "y_pred = predicted_values_df.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# If you also need true values\n",
    "true_values_df = predictions.select(\"label\")\n",
    "y_true = true_values_df.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Display some results\n",
    "print(f\"First few true values: {y_true[:5]}\")\n",
    "print(f\"First few predicted values: {y_pred[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648aa91-586b-412e-8917-1f847ae91a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(12, 8))  # Width = 12 inches, Height = 8 inches\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(y_true, y_pred, c='blue', label='Actual vs. Predicted')\n",
    "plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], linestyle='--', color='red', label='Prediction')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Values (y_true)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "plt.title('Scatter Plot of Actual vs. Predicted Values')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20805987-a6da-430c-a438-956a32daa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "\n",
    "\n",
    "# Define the HDFS path where the model is saved\n",
    "model_path = \"hdfs://localhost:9000/user/talentum/lrg_best_model\"  # Replace with your HDFS path\n",
    "\n",
    "# Define the artificial input data with all 16 features\n",
    "input_data = np.array([[31, 17, 11, 6, 20, 20714, 18, 18, 18, 5, 1, 63, 0, 1014, 10, 82]])\n",
    "\n",
    "# Convert input_data to a Spark DataFrame\n",
    "feature_columns = [f\"feature{i+1}\" for i in range(input_data.shape[1])]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(input_data, columns=feature_columns))\n",
    "\n",
    "# Load the trained model from HDFS\n",
    "model = LinearRegressionModel.load(model_path)\n",
    "\n",
    "# Prepare the feature assembler\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "input_data_transformed = assembler.transform(input_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(input_data_transformed)\n",
    "\n",
    "# Show predictions (for demonstration purposes)\n",
    "predictions.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35fd6e-25f4-483b-8bf8-f48bf04d83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the HDFS path\n",
    "hdfs_path = \"hdfs://localhost:9000/user/talentum/\"  # Replace with your actual HDFS path\n",
    "\n",
    "# Load the training and testing sets from HDFS\n",
    "train_df = spark.read.parquet(f'{hdfs_path}/train_df')\n",
    "test_df = spark.read.parquet(f'{hdfs_path}/test_df')\n",
    "\n",
    "# Prepare the feature assembler\n",
    "assembler = VectorAssembler(inputCols=[col for col in train_df.columns if col != 'tempC'], outputCol=\"features\")\n",
    "\n",
    "# Transform the data to include the feature column\n",
    "train_data_assembled = assembler.transform(train_df)\n",
    "test_data_assembled = assembler.transform(test_df)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_data_assembled)\n",
    "train_data_scaled = scaler_model.transform(train_data_assembled)\n",
    "test_data_scaled = scaler_model.transform(test_data_assembled)\n",
    "\n",
    "# Prepare final training and test datasets\n",
    "train_data_final = train_data_scaled.select(col(\"scaled_features\").alias(\"features\"), col(\"tempC\").alias(\"label\"))\n",
    "test_data_final = test_data_scaled.select(col(\"scaled_features\").alias(\"features\"), col(\"tempC\").alias(\"label\"))\n",
    "\n",
    "# Initialize the Decision Tree Regressor model\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Set up the parameter grid for tuning only maxDepth and minInfoGain\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [5, 10, 15])   # Test different tree depths\n",
    "             .addGrid(dt.minInfoGain, [0.0, 0.01])  # Test different thresholds for info gain\n",
    "             .build())\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up cross-validator\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,      # 3-fold cross-validation\n",
    "                          parallelism=4)   # Use 4 parallel tasks\n",
    "\n",
    "# Fit the cross-validator model\n",
    "cv_model = crossval.fit(train_data_final)\n",
    "\n",
    "# Select the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Evaluate the best model on test data\n",
    "predictions = best_model.transform(test_data_final)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Best Model's RMSE on test data: {rmse}\")\n",
    "\n",
    "# Define model paths\n",
    "model_path_hdfs = f'{hdfs_path}dt_model'\n",
    "scaler_model_path_hdfs = f'{hdfs_path}scaler_model'\n",
    "\n",
    "# Save the best Decision Tree model to HDFS\n",
    "best_model.write().overwrite().save(model_path_hdfs)\n",
    "print(f\"Decision Tree model saved to: {model_path_hdfs}\")\n",
    "\n",
    "# Save the Standard Scaler model to HDFS\n",
    "scaler_model.write().overwrite().save(scaler_model_path_hdfs)\n",
    "print(f\"Scaler model saved to: {scaler_model_path_hdfs}\")\n",
    "\n",
    "# Evaluate the model using MAE\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE) on test data = {mae}\")\n",
    "\n",
    "# Show some prediction results\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bc3a1-12a3-46cc-a118-5a841d9acabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using MSE\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074ef04-e91b-43a5-bb54-81b60b499a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using R²\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R² score on test data = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9f960-f990-4ee0-9a6d-42e5729e95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScalerModel\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Define the HDFS path\n",
    "hdfs_model_path = \"hdfs://localhost:9000/user/talentum/dt_model\"\n",
    "hdfs_scaler_path = \"hdfs://localhost:9000/user/talentum/scaler_model\"\n",
    "\n",
    "# Load the trained model from HDFS\n",
    "dt_model = DecisionTreeRegressionModel.load(hdfs_model_path)\n",
    "\n",
    "# Load the scaler model from HDFS\n",
    "scaler_model = StandardScalerModel.load(hdfs_scaler_path)\n",
    "\n",
    "# Prepare the input data\n",
    "input_data = np.array([[31, 17, 11, 6, 20, 20714, 18, 18, 18, 5, 1, 63, 0, 1014, 10, 82]])\n",
    "input_df = spark.createDataFrame([Row(features=Vectors.dense(input_data[0]))])\n",
    "\n",
    "# Scale the input data\n",
    "input_data_scaled = scaler_model.transform(input_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = dt_model.transform(input_data_scaled)\n",
    "predictions.select(\"prediction\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89b9d1-e06f-4cf7-aaa0-f6c0dd5335bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Set up the parameter grid for tuning numTrees, maxDepth, and minInfoGain\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [10, 20, 50])\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "             .addGrid(rf.minInfoGain, [0.0, 0.01, 0.1])\n",
    "             .build())\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up cross-validator\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,      # 3-fold cross-validation\n",
    "                          parallelism=4)   # Use 4 parallel tasks\n",
    "\n",
    "# Fit the cross-validator model\n",
    "cv_model = crossval.fit(train_data_final)\n",
    "\n",
    "# Select the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Save the best Random Forest model to HDFS\n",
    "model_path_hdfs = f'{hdfs_path}rf_model'\n",
    "scaler_model_path_hdfs = f'{hdfs_path}scaler_model'\n",
    "best_model.write().overwrite().save(model_path_hdfs)\n",
    "print(f\"Random Forest model saved to: {model_path_hdfs}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(test_data_final)\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(f\"Best Model's RMSE on test data: {rmse}\")\n",
    "\n",
    "# Evaluate the model using MAE\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE) on test data = {mae}\")\n",
    "\n",
    "# Evaluate the model using R-squared (R2)\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R-squared (R2) on test data = {r2}\")\n",
    "\n",
    "# Show some prediction results\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe402bb0-96af-4f0c-a5b9-acecd08a3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Reduced parameter grid for quicker training\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [10, 20])\n",
    "             .addGrid(rf.maxDepth, [5, 10])\n",
    "             .addGrid(rf.minInfoGain, [0.0])\n",
    "             .build())\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up cross-validator with reduced number of folds and increased parallelism\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2,      # Reduced number of folds\n",
    "                          parallelism=8)   # Increased parallelism\n",
    "\n",
    "# Sample data to speed up the process\n",
    "sample_fraction = 0.5  # Use 50% of the data\n",
    "train_data_sample = train_data_final.sample(withReplacement=False, fraction=sample_fraction)\n",
    "\n",
    "# Fit the cross-validator model\n",
    "cv_model = crossval.fit(train_data_sample)\n",
    "\n",
    "# Select the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Save the best Random Forest model to HDFS\n",
    "model_path_hdfs = f'{hdfs_path}rf_model'\n",
    "scaler_model_path_hdfs = f'{hdfs_path}scaler_model'\n",
    "best_model.write().overwrite().save(model_path_hdfs)\n",
    "print(f\"Random Forest model saved to: {model_path_hdfs}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(test_data_final)\n",
    "\n",
    "# Evaluate the best model\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Best Model's RMSE on test data: {rmse}\")\n",
    "\n",
    "# Additional metrics\n",
    "mae = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(predictions)\n",
    "r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE) on test data = {mae}\")\n",
    "print(f\"R-squared (R2) on test data = {r2}\")\n",
    "\n",
    "# Show some prediction results\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f8ad9-1321-4e60-8c5b-422386eb21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"file:///home/talentum/Pune.csv\")\n",
    "\n",
    "# Drop unsupported columns\n",
    "data = data.drop(columns=['date_time', 'moonrise', 'moonset', 'sunrise', 'sunset', 'FeelsLikeC'])\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(columns=['tempC'])  # Features\n",
    "y = data['tempC']                 # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "\n",
    "# Define a smaller parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with a smaller grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,              # 3-fold cross-validation\n",
    "    verbose=1,         # Print progress\n",
    "    n_jobs=-1          # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Train the model with hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE) on test data: {mae}\")\n",
    "print(f\"R-squared (R2) on test data: {r2}\")\n",
    "\n",
    "# Show some prediction results\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf1878-85d2-48d4-b7a7-b443fbbb1bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
